{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Current Path: /Users/jinwei/Documents/GitHub/point_morph\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import socket\n",
    "import os\n",
    "import sys\n",
    "import provider_zj\n",
    "BASE_DIR = os.getcwd()\n",
    "print('The Current Path:', BASE_DIR)\n",
    "import h5py\n",
    "sys.path.append(os.path.join(BASE_DIR, 'model'))\n",
    "sys.path.append(os.path.join(BASE_DIR,'utils'))\n",
    "import transform_nets\n",
    "import tf_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# para set\n",
    "BATCH_SIZE = 32\n",
    "NUM_POINT = 1024 \n",
    "MAX_EPOCH = 200\n",
    "BASE_LEARNING_RATE = 0.001\n",
    "GPU_INDEX = 0\n",
    "MOMENTUM = 0.9\n",
    "OPTIMIZER = 'adam'\n",
    "DECAY_STEP = 200000\n",
    "DECAY_RATE = 0.7\n",
    "LOG_DIR = 'log'\n",
    "#\n",
    "BN_INIT_DECAY = 0.5\n",
    "BN_DECAY_DECAY_RATE = 0.5\n",
    "BN_DECAY_DECAY_STEP = float(DECAY_STEP)\n",
    "BN_DECAY_CLIP = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mkdir log file\n",
    "if not os.path.exists(LOG_DIR): os.mkdir(LOG_DIR)\n",
    "LOG_FOUT = open(os.path.join(LOG_DIR, 'log_train.txt'),'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_string(out_str):\n",
    "    LOG_FOUT.write(out_str+'\\n')\n",
    "    LOG_FOUT.flush()\n",
    "    print(out_str)\n",
    "\n",
    "def get_learning_rate(batch):\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "                        BASE_LEARNING_RATE,  # Base learning rate.\n",
    "                        batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "                        DECAY_STEP,          # Decay step.\n",
    "                        DECAY_RATE,          # Decay rate.\n",
    "                        staircase=True)\n",
    "    learing_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!\n",
    "    return learning_rate \n",
    "\n",
    "def get_bn_decay(batch):\n",
    "    bn_momentum = tf.train.exponential_decay(\n",
    "                      BN_INIT_DECAY,\n",
    "                      batch*BATCH_SIZE,\n",
    "                      BN_DECAY_DECAY_STEP,\n",
    "                      BN_DECAY_DECAY_RATE,\n",
    "                      staircase=True)\n",
    "    bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
    "    return bn_decay "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_global_feature_net(points,is_training_pl,bn_decay):\n",
    "    batch_size = points.get_shape()[0].value\n",
    "    num_points = points.get_shape()[1].value\n",
    "    end_points = {}\n",
    "    with tf.variable_scope('transform_net1') as sc:\n",
    "        transform = transform_nets.input_transform_net(points, is_training_pl,bn_decay,K=3)\n",
    "    points_transformed = tf.matmul(points, transform)\n",
    "    input_image = tf.expand_dims(points_transformed,-1)\n",
    "    \n",
    "    net = tf_util.conv2d(input_image, 64, [1,3], \n",
    "                        padding='VALID',stride=[1,1],\n",
    "                        bn=True, is_training=is_training_pl,\n",
    "                        scope='conv1',bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 64, [1,1], padding='VALID',\n",
    "                        stride=[1,1], bn=True, is_training=is_training_pl,\n",
    "                        scope='conv2', bn_decay=bn_decay)\n",
    "    with tf.variable_scope('transform_net2') as sc:\n",
    "        transform = transform_nets.feature_transform_net(net, is_training_pl, bn_decay, K=64)\n",
    "    end_points['transform'] = transform\n",
    "    net_transformed = tf.matmul(tf.squeeze(net), transform)\n",
    "    net_transformed = tf.expand_dims(net_transformed,[2])\n",
    "    \n",
    "    points_feature = net_transformed\n",
    "    \n",
    "    net = tf_util.conv2d(net_transformed, 64, [1,1], padding='VALID',\n",
    "                        stride=[1,1], bn=True, is_training=is_training_pl,\n",
    "                        scope='conv3', bn_decay=bn_decay)\n",
    "    \n",
    "    net = tf_util.conv2d(net_transformed, 128, [1,1], padding='VALID',\n",
    "                        stride=[1,1], bn=True, is_training=is_training_pl,\n",
    "                        scope='conv4', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net_transformed, 1024, [1,1], padding='VALID',\n",
    "                        stride=[1,1], bn=True, is_training=is_training_pl,\n",
    "                        scope='conv5', bn_decay=bn_decay)\n",
    "    net = tf_util.max_pool2d(net,[num_points,1], padding='VALID', \n",
    "                            scope='maxpooling')\n",
    "    print(net)\n",
    "    net = tf.reshape(net, [batch_size, -1])\n",
    "    \n",
    "    return net, points_feature, end_points\n",
    "\n",
    "\n",
    "def get_morph_net(target_global_feature,source_global_feature,source_points_features,is_training_pl):\n",
    "    num_point = source_points_features.get_shape()[1].value\n",
    "    target_global_feature = tf.expand_dims(tf.expand_dims(target_global_feature,dim =1),dim=2)\n",
    "    source_global_feature = tf.expand_dims(tf.expand_dims(source_global_feature,dim =1),dim=2)\n",
    "\n",
    "\n",
    "    print(target_global_feature)\n",
    "    target_global_feature_expand = tf.tile(target_global_feature,[1, num_point ,1,1])\n",
    "    target_global_feature_expand = tf.tile(source_global_feature,[1, num_point ,1,1])\n",
    "    concat_feat = tf.concat([source_points_features,\n",
    "                               target_global_feature_expand,target_global_feature_expand],axis=3)\n",
    "    net = tf_util.conv2d(concat_feat, 512, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training_pl,\n",
    "                         scope='conv1', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 256, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training_pl,\n",
    "                         scope='conv2', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 128, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training_pl,\n",
    "                         scope='conv3', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 64, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training_pl,\n",
    "                         scope='conv4', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 3, [1,1],\n",
    "                         padding='VALID', stride=[1,1], activation_fn=None,\n",
    "                         scope='conv10')\n",
    "\n",
    "    net = tf.squeeze(net, [2]) # BxNx3\n",
    "    return net\n",
    "\n",
    "    def get_loss_func(target_global_feature,source_to_target_feature,source_to_traget_points,source_points):\n",
    "        \n",
    "        margin = 0.2\n",
    "        d_eucd2 = tf.pow(tf.subtract(source_points,source_to_traget_points),2)\n",
    "        d_eucd2 = tf.reduce_sum(d_eucd2, 1)\n",
    "        d_eucd = tf.sqrt(d_eucd2+1e-6, name=\"d_eucd\")\n",
    "        C = tf.constant(margin, name=\"C\")\n",
    "        d_eucd = tf.maximum(tf.subtract(d_eucd,C),0)\n",
    "        \n",
    "        f_eucd2 = tf.pow(tf.subtract(target_global_feature,source_to_target_feature),2)\n",
    "        f_eucd2 = tf.reduce_sum(eud2,1)\n",
    "        f_eucd = tf.sqrt(eucd2+1e-6, name=\"f_eucd\")\n",
    "        losses = tf.add(d_eucd, f_eucd, name=\"losses\")\n",
    "        loss = tf.reduce_mean(losses, name=\"loss\")\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-13-1e4b8d5903d0>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-1e4b8d5903d0>\"\u001b[0;36m, line \u001b[0;32m25\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def train_one_epoch(sess,ops,train_writer):\n",
    "    is_traning_pl = True\n",
    "    train_file_idxs = np.arrange(0,len(Train_FILES))\n",
    "    np.random.shuffle(train_file_idxs)\n",
    "    \n",
    "    for fn in range(len(Train_FILES)):\n",
    "        log_string('----'+str(fn)+'----')\n",
    "        current_points1, current_points2 = provider_zj.load_h5(Train_FILES[train_file_idxs[fn]])\n",
    "        current_points1 = current_points1[:,0:NUM_POINT,:]\n",
    "        current_points2 = current_points2[:,0:NUM_POINT,:]\n",
    "        file_size = current_points1.shape[0]\n",
    "        num_batches = file_size/BATCH_SIZE\n",
    "        loss_sum = 0\n",
    "        \n",
    "        for batch_idx in range(num_batches-1):\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"features/maxpooling:0\", shape=(32, 1, 1, 1024), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"features/maxpooling_1:0\", shape=(32, 1, 1, 1024), dtype=float32, device=/device:GPU:0)\n",
      "(32, 1024, 1, 64)\n",
      "Tensor(\"morph/ExpandDims_1:0\", shape=(32, 1, 1, 1024), dtype=float32, device=/device:GPU:0)\n"
     ]
    }
   ],
   "source": [
    "# train files load\n",
    "TRAIN_FILES = provider_zj.getDataFiles('/Users/jinwei/Dataset/corres_1024/train_files.txt')\n",
    "with tf.Graph().as_default():\n",
    "    with tf.device('/gpu:'+str(GPU_INDEX)):\n",
    "        is_training_pl = True\n",
    "        points1 = tf.placeholder(tf.float32, shape=(BATCH_SIZE,NUM_POINT,3))\n",
    "        points2 = tf.placeholder(tf.float32, shape=(BATCH_SIZE,NUM_POINT,3))\n",
    "        batch = tf.Variable(0)\n",
    "        bn_decay = get_bn_decay(batch)\n",
    "        tf.summary.scalar('bn_decay',bn_decay)\n",
    "        with tf.variable_scope('features') as sc:\n",
    "            # target\n",
    "            net1, points_feature1, end_points1 = extract_global_feature_net(points1,is_training_pl,bn_decay)\n",
    "                                                                        \n",
    "            sc.reuse_variables()\n",
    "            # source\n",
    "            net2, points_feature2, end_points2 = extract_global_feature_net(points2,is_training_pl,bn_decay)\n",
    "            \n",
    "                                                                            \n",
    "        print(points_feature2.get_shape())                                                                \n",
    "                                                                            \n",
    "        with tf.variable_scope('morph') as sc:\n",
    "            get_morph_net(net1,net2,points_feature2,is_training_pl)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"features/Reshape:0\", shape=(32, 1024), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"ExpandDims_1:0\", shape=(32, 1, 1, 1024), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(net1)\n",
    "print(tf.expand_dims(tf.expand_dims(net1,dim =1),dim=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
